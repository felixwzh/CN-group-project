\documentclass[sigconf]{acmart}

\usepackage{booktabs} % For formal tables
\usepackage{url}
\usepackage{graphicx}
\usepackage{subcaption}
\newcommand{\kai}[1]{{\bf \color{blue} [[Shukai says ``#1'']]}}
\newcommand{\heng}[1]{{\bf \color{cyan} [[Yuheng says ``#1'']]}}
\newcommand{\hao}[1]{{\bf \color{red} [[Hao says ``#1'']]}}
\newcommand{\hui}[1]{{\bf \color{purple} [[Zhenghui says ``#1'']]}}
% Copyright
%\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


% DOI
%\acmDOI{10.475/123_4}

% ISBN
%\acmISBN{123-4567-24-567/08/06}

%Conference
\acmConference[CNW]{SJTU Computer Network Workshop}{December 2017}{Shanghai, China} 
\acmYear{2017}
\copyrightyear{2017}


%\acmArticle{4}
%\acmPrice{15.00}

% These commands are optional
%\acmBooktitle{Transactions of the ACM Woodstock conference}
%\editor{Jennifer B. Sartor}
%\editor{Theo D'Hondt}
%\editor{Wolfgang De Meuter}


\begin{document}
\title{Predicting Internet Path Dynamics and Performance with Machine Learning}
%\titlenote{Produces the permission block, and
%  copyright information}
%\subtitle{Extended Abstract}
%\subtitlenote{The full version of the author's guide is available as
%  \texttt{acmart.pdf} document}


\author{Zhenghui Wang}
\affiliation{%
  \institution{Shanghai Jiao Tong University}
  \city{Shanghai} 
  \state{China} 
  \postcode{200240}
}
\email{felixwzh@outlook.com}

\author{Hao Wang}
\affiliation{%
	\institution{Shanghai Jiao Tong University}
	\city{Shanghai} 
	\state{China} 
	\postcode{200240}
}
\email{?@?.com}

\author{Yuheng Zhi}
\affiliation{%
	\institution{Shanghai Jiao Tong University}
	\city{Shanghai} 
	\state{China} 
	\postcode{200240}
}
\email{?@?.com}

\author{Shukai Liu}
\affiliation{%
	\institution{Shanghai Jiao Tong University}
	\city{Shanghai} 
	\state{China} 
	\postcode{200240}
}
\email{?@?.com}

% The default list of authors is too long for headers.
\renewcommand{\shortauthors}{CN Group}


\begin{abstract}
We study the problem of predicting internet path dynamics and performance. We use \texttt{traceroute} measurement and machine learning models.
\end{abstract}

%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below. 
%
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10010520.10010553.10010562</concept_id>
  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010575.10010755</concept_id>
  <concept_desc>Computer systems organization~Redundancy</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010553.10010554</concept_id>
  <concept_desc>Computer systems organization~Robotics</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>10003033.10003083.10003095</concept_id>
  <concept_desc>Networks~Network reliability</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>  
\end{CCSXML}

%\ccsdesc[500]{Computer systems organization~Embedded systems}
%\ccsdesc[300]{Computer systems organization~Redundancy}
%\ccsdesc{Computer systems organization~Robotics}
%\ccsdesc[100]{Networks~Network reliability}


\keywords{TODO}


  \maketitle
 
\hui{I think we should focus on the process not the final result, which is also important.}
 
\section{Introduction}
 
1. introduce the original paper: their task, method, and dataset
 
Internet paths change frequently due to inter/intra-domain routing changes, load balancing, and even misconfigurations and failures. Some of these changes can seriously disrupt performance, causing longer round-trip times, congestion, or even loss of connectivity. Thus, it is of great significant to predict Internet path dynamics and performance.
 

In the original paper, the authors focus on the problem of predicting Internet path changes and path performance using \texttt{traceroute} measurements. They uses the recent route information of paths (route age for paths, route changes in past, average RTT, etc) to do some predictions of paths. Their are three predict targets: $(i)$ the remaining life time of a path (i.e., the time before a path changes), $(ii)$ the number of path changes in a future timeslot, and $(iii)$ the average RTT of a path in the next \texttt{traceroute} measurement.
 

To achieve these goals, it introduces a NETPerfTrace system, which relies on a standard random forest model for prediction. Moreover it uses extensive evaluation on the impact of different input features by studying the correlations between the inputs and the prediction targets, as well as target selection techniques.
 

The dataset it provides with is a full week of Paris \texttt{traceroute} measurements performed through the M-Lab open
 
Internet measurement initiative. The author observes more than 450,000 different paths sampled through Paris-traceroute measurements from more than 180 geo-distributed servers. However, most of the paths are not periodically sampled during this week. And only 2,346 paths have at least 100 \texttt{traceroute} measurements during the analyzed week. So the dataset only contains corresponding infomation of these 2,346 paths.
 

We find some drawbacks in their data process.
 
The author processes the data and calculates some statistics for the whole seven days and then use the data to get the ramdon forest model.
 

\hao{maybe we can put the detail discussion of their data process problems in data analysis part and delete the above sentence only leave the first one.}
 

And in our own work,firstly we reprocess the provided data in more resonable ways and use the same method (random forest) mentioned by the author to compare the results. Secondly, two other machine learning methods , xgboost and LSTM, are applied to get better performance.
 

\section{Related Works}
 
1. introduce the original paper
 
In the original paper, the problem they solve is to use random forest model to predict three labels for one path: $(i)$ the remaining life time of a path (i.e., the time before a path changes), $(ii)$ the number of path changes in a future timeslot, and $(iii)$ the average RTT of a path in the next \texttt{traceroute} measurement.
 

And to achieve these three targets, 69 input features are used to describe the  statistical properties of route dynamics and path latency.
 
For the first route dynamic target, the first group of 11 features,referred to as $F_A$, is chosen. It describes the statistical properties (average, minimum, maximum, and percentiles) of the route duration observed for each path.
 
And for the second target, the second group of 14 features, as $F_B$, are relevant to the prediction of number of route changes. $F_B$ features take into account the statistical properties of route changes. In addition to that, $F_B$ contains information about the number of route changes observed for a path so far and a binary feature indicating whether a route change occurred for a path in the current time slot. 
 
And the last group of 44 features ,referred to as $F_C$ decribes statistical properties of path latency, which is relavent to the prediction of the next \texttt{traceroute} measurement. They calculate the staistical properties of four RTT metrics (average, minimum, maximum and standard deviation) reported from each \texttt{traceroute} measurement.
 

Then they study the correlation among the input features and the targets and apply  feature selection techniques to select the best features for prediction. When using as input the full set of 69 input features $F_A \cup F_B \cup F_C$, and perform wrapper-based feature selection on top of this full set. The result shows that the top important features for three targets are not necessarily the ones in corresponding $F_A,F_B$ or $F_C$.
 

And the authors use three different ways of selecting input features: $(i)$  use all of the 69 features to predict each target, $(ii)$ when predicting target $X$, use corresponding feature set $F_X$, $(iii)$ after wrapper-based feature selection, use the top features for each target. And finally, what they get is the third way of choosing input features can achieve better results (though there is only minor differences in output results).
 


2. we can introduce some other machine learning methods applied in computer network scenarios. \hui{I'll take this part}
 


3. very briefly introduce xgboost and lstm
 

\section{Data Analysis}
\hui{I think this is an important part, who will take this part?}
\heng{Plots of temporally differentiated prediction targets are given here to show their behavior patterns in temporal domain, thus supporting the use of LSTMs.}
With careful review of the data used in the original paper, here we (a) show the authors had used the data incorrectly in the context of making \textbf{predictions} and (b) reveal the temporal properties that the original method failed to employ but perfectly fits LSTMs (long short term memory neural networks), which we later use to improve the performance.

\subsection{Justification of Data Processing}
The original paper \cite{predict} made a major mistake on data employment: they implicitly used data from the future to predict events in the future, which is not possible. More specifically, all of their tests on the performance of \textit{NetPerfTrace} depend on statistics calculated out of data of all time, including the data they were to predict. Features like \{5, 15, 25...\} percentage point of route life, for example, takes the route life of all routes during the whole week into account, including route $r_i$. However, these features still take part in predicting the residual life of $r_i$. This is just like guessing something when you knew it.

In fact, a principle of making predictions is to use the data \textbf{already observed} to guess the unobserved. Thus, in all the experiments of this paper, we made it a hard constraint that all the features used to predict any event at, say, time $t$, only come from the raw data collected \textbf{before} $t$. First, we re-validated the performance of classic machine learning methods on the 3 tasks according to our justified data processing manner, in the following 3 ways: (a) use the global features (max value, min value, distribution, etc.) of the former k\% data to train the models and predict the targets of the latter (1-k)\% data, (b) use the global features to train the models and update them during the predicting procedure and (c) use data in a certain recent time-slot to calculate the global features. Then we train LSTMs on these data in two different ways: (a) input only a scalar, the prediction target, to the network and (b) input the prediction target and the global features as a vector.

\subsection{Temporal Properties of the Data}
We believe temporally adjacent routes and \textbf{traceroute} records of a path share some common network nodes, thus their behavior contain implicit dependencies in temporal domain. While the data are organized in the order of time, the original method ignored this property but regard them as independent instances, i.e., the prediction for the next time step only depends on the features of the present, but does not use information given by more previous time steps. This manner fails the original method to capture even the most naive temporal dependencies. Here we do not provide strict proofs of the temporal dependencies that lie in these routes but simply throw some light on the possibility of their existence.

\begin{figure}[tp]
	\centering
	\begin{subfigure}[t]{0.4\linewidth}
		\centering
		\includegraphics[width=0.95\linewidth, trim={100 100 0 0}, clip=true]{fig/total_life_8}
		\label{fig:3.1a}
	\end{subfigure}
	\begin{subfigure}[t]{0.4\linewidth}
		\centering
		\includegraphics[width=0.95\linewidth, trim={100 100 0 0}, clip]{fig/total_life_20}
		\label{fig:3.1b}
	\end{subfigure}
	\begin{subfigure}[t]{0.4\linewidth}
		\centering
		\includegraphics[width=0.95\linewidth, trim={100 100 0 0}, clip=true]{fig/total_life_21}
		\label{fig:3.1c}
	\end{subfigure}
	\begin{subfigure}[t]{0.4\linewidth}
		\centering
		\includegraphics[width=0.95\linewidth, trim={100 100 0 0}, clip]{fig/total_life_30}
		\label{fig:3.1d}
	\end{subfigure}
	\caption{Temporal patterns of route life among 4 different paths.}
	\label{fig:3.1}
\end{figure}

\begin{figure}[!htp]
	\centering
	\begin{subfigure}[t]{0.4\linewidth}
		\centering
		\includegraphics[width=0.95\linewidth, trim={100 100 0 0}, clip=true]{fig/route_change_8}
		\label{fig:3.2a}
	\end{subfigure}
	\begin{subfigure}[t]{0.4\linewidth}
		\centering
		\includegraphics[width=0.95\linewidth, trim={100 100 0 0}, clip]{fig/route_change_20}
		\label{fig:3.2b}
	\end{subfigure}
	\begin{subfigure}[t]{0.4\linewidth}
		\centering
		\includegraphics[width=0.95\linewidth, trim={100 100 0 0}, clip=true]{fig/route_change_21}
		\label{fig:3.2c}
	\end{subfigure}
	\begin{subfigure}[t]{0.4\linewidth}
		\centering
		\includegraphics[width=0.95\linewidth, trim={100 100 0 0}, clip]{fig/route_change_30}
		\label{fig:3.2d}
	\end{subfigure}
	\caption{Temporal patterns of the number of route changes among 4 different paths.}
	\label{fig:3.2}
\end{figure}

\begin{figure}[!htp]
	\centering
	\begin{subfigure}[t]{0.4\linewidth}
		\centering
		\includegraphics[width=0.95\linewidth, trim={100 100 0 0}, clip=true]{fig/RTT_8}
		\label{fig:3.3a}
	\end{subfigure}
	\begin{subfigure}[t]{0.4\linewidth}
		\centering
		\includegraphics[width=0.95\linewidth, trim={100 100 0 0}, clip]{fig/RTT_20}
		\label{fig:3.3b}
	\end{subfigure}
	\begin{subfigure}[t]{0.4\linewidth}
		\centering
		\includegraphics[width=0.95\linewidth, trim={100 100 0 0}, clip=true]{fig/RTT_21}
		\label{fig:3.3c}
	\end{subfigure}
	\begin{subfigure}[t]{0.4\linewidth}
		\centering
		\includegraphics[width=0.95\linewidth, trim={100 100 0 0}, clip]{fig/RTT_30}
		\label{fig:3.3d}
	\end{subfigure}
	\caption{Temporal patterns of round trip time among 4 different paths.}
	\label{fig:3.3}
\end{figure}

Applying simple differential analysis to the raw data, we found the prediction targets indeed show local temporal patterns which we can hopefully capture with LSTMs. Fig.~\ref{fig:3.1} to Fig.~\ref{fig:3.3} exhibit differentiated results of the three prediction targets in different paths. We can see that these paths behave in somehow consistent temporal patterns along their own ways but differently from each other.

No more solid evidence could be provided to prove the periodicity of these patterns because \textbf{pattern} itself is hard to define. However, these results are already enough to make us hopeful about the performance of LSTMs, which are expert in capturing temporal dependencies, no-matter long-term ones or short-term ones.


1. We can plot some figures of the statistics of data, like the distribution of the route duration and avgRTT. 

2. We can further discuss the relation between routes in one path or in different paths.

3. Then we could discuss why the authors of the paper process the data in a wrong way

4. We show our solution for data process. three ways for random forest models.


\section{Methodology}
\subsection{Data Processing}


\subsection{LSTM Model}

\section{Experiment}
\subsection{Classic Models}
We show the experiment results of the 3 different data we obtained, namely \textbf{K\&fix},\textbf{K\&update},\textbf{timeslot\&update}. We need to find some difference between our 3 data processing methods and the authors', i.e.,\textbf{origin}. 

The experiments we need to conduct are as follows:
\begin{itemize}

\item 1. \textbf{K\&fix}+RF
\item 2. \textbf{K\&update}+RF
\item 3. \textbf{timeslot\&update}+RF
\item 4. \textbf{origin}+RF
\item 5. \textbf{K\&fix}+xgBoost
\item 6. \textbf{K\&update}+xgBoost
\item 7. \textbf{timeslot\&update}+xgBoost
\item 8. \textbf{origin}+xgBoost
\end{itemize}


\hui{Note}We first predict the route duration instead of resLife. Further study on the difference between these two predicting objective could be conduct if we have time. Currently, we predict the route duration for task 1, because we can only predict this when we use LSTM.


\subsection{Deep Models}

There are two kinds of input for the LSTM at each timestep, (i) one simple scalar (ii) a vector.
The experiments we need to conduct are as follows:
\begin{itemize}
\item scalar input + LSTM
\item vector input + LSTM
\end{itemize}

\section{Conclusions}
TODO

\begin{acks}
TODO
\end{acks}



\bibliographystyle{ACM-Reference-Format}
\bibliography{bibliography} 

\end{document}
=======
\documentclass[sigconf]{acmart}

\usepackage{booktabs} % For formal tables
\usepackage{url}
\newcommand{\kai}[1]{{\bf \color{blue} [[Shukai says ``#1'']]}}
\newcommand{\heng}[1]{{\bf \color{cyan} [[Yuheng says ``#1'']]}}
\newcommand{\hao}[1]{{\bf \color{red} [[Hao says ``#1'']]}}
\newcommand{\hui}[1]{{\bf \color{purple} [[Zhenghui says ``#1'']]}}
% Copyright
%\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


% DOI
%\acmDOI{10.475/123_4}

% ISBN
%\acmISBN{123-4567-24-567/08/06}

%Conference
\acmConference[CNW]{SJTU Computer Network Workshop}{December 2017}{Shanghai, China} 
\acmYear{2017}
\copyrightyear{2017}


%\acmArticle{4}
%\acmPrice{15.00}

% These commands are optional
%\acmBooktitle{Transactions of the ACM Woodstock conference}
%\editor{Jennifer B. Sartor}
%\editor{Theo D'Hondt}
%\editor{Wolfgang De Meuter}


\begin{document}
	\title{Predicting Internet Path Dynamics and Performance with Machine Leanring}
	%\titlenote{Produces the permission block, and
	%  copyright information}
	%\subtitle{Extended Abstract}
	%\subtitlenote{The full version of the author's guide is available as
	%  \texttt{acmart.pdf} document}
	
	
	\author{Zhenghui Wang}
	\affiliation{%
		\institution{Shanghai Jiao Tong University}
		\city{Shanghai} 
		\state{China} 
		\postcode{200240}
	}
	\email{felixwzh@outlook.com}
	
	\author{Hao Wang}
	\affiliation{%
		\institution{Shanghai Jiao Tong University}
		\city{Shanghai} 
		\state{China} 
		\postcode{200240}
	}
	\email{?@?.com}
	
	\author{Yuheng Zhi}
	\affiliation{%
		\institution{Shanghai Jiao Tong University}
		\city{Shanghai} 
		\state{China} 
		\postcode{200240}
	}
	\email{?@?.com}
	
	\author{Shukai Liu}
	\affiliation{%
		\institution{Shanghai Jiao Tong University}
		\city{Shanghai} 
		\state{China} 
		\postcode{200240}
	}
	\email{?@?.com}
	
	% The default list of authors is too long for headers.
	\renewcommand{\shortauthors}{CN Group}
	
	
	\begin{abstract}
		We study the problem of predicting internet path dynamics and performance. We use \texttt{traceroute} measurement and machine learning models.
	\end{abstract}
	
	%
	% The code below should be generated by the tool at
	% http://dl.acm.org/ccs.cfm
	% Please copy and paste the code instead of the example below. 
	%
	\begin{CCSXML}
		<ccs2012>
		<concept>
		<concept_id>10010520.10010553.10010562</concept_id>
		<concept_desc>Computer systems organization~Embedded systems</concept_desc>
		<concept_significance>500</concept_significance>
		</concept>
		<concept>
		<concept_id>10010520.10010575.10010755</concept_id>
		<concept_desc>Computer systems organization~Redundancy</concept_desc>
		<concept_significance>300</concept_significance>
		</concept>
		<concept>
		<concept_id>10010520.10010553.10010554</concept_id>
		<concept_desc>Computer systems organization~Robotics</concept_desc>
		<concept_significance>100</concept_significance>
		</concept>
		<concept>
		<concept_id>10003033.10003083.10003095</concept_id>
		<concept_desc>Networks~Network reliability</concept_desc>
		<concept_significance>100</concept_significance>
		</concept>
		</ccs2012>  
	\end{CCSXML}
	
	%\ccsdesc[500]{Computer systems organization~Embedded systems}
	%\ccsdesc[300]{Computer systems organization~Redundancy}
	%\ccsdesc{Computer systems organization~Robotics}
	%\ccsdesc[100]{Networks~Network reliability}
	
	
	\keywords{TODO}
	
	
	\maketitle
	\hui{I think we should focus on the process not the final result, which is also important.}
	\section{Introduction}
	1. introduce the original paper: their task, method, and dataset
	Internet paths change frequently due to inter/intra-domain routing changes, load balancing, and even misconfigurations and failures. Some of these changes can seriously disrupt performance, causing longer round-trip times, congestion, or even loss of connectivity. Thus, it is of great significant to predict Internet path dynamics and performance.
	
	In the original paper, the authors focus on the problem of predicting Internet path changes and path performance using \texttt{traceroute} measurements. They uses the recent route information of paths (route age for paths, route changes in past, average RTT, etc) to do some predictions of paths. Their are three predict targets: $(i)$ the remaining life time of a path (i.e., the time before a path changes), $(ii)$ the number of path changes in a future timeslot, and $(iii)$ the average RTT of a path in the next \texttt{traceroute} measurement.
	
	To achieve these goals, it introduces a NETPerfTrace system, which relies on a standard random forest model for prediction. Moreover it uses extensive evaluation on the impact of different input features by studying the correlations between the inputs and the prediction targets, as well as target selection techniques.
	
	The dataset it provides with is a full week of Paris \texttt{traceroute} measurements performed through the M-Lab open
	Internet measurement initiative. The author observes more than 450,000 different paths sampled through Paris-traceroute measurements from more than 180 geo-distributed servers. However, most of the paths are not periodically sampled during this week. And only 2,346 paths have at least 100 \texttt{traceroute} measurements during the analyzed week. So the dataset only contains corresponding infomation of these 2,346 paths.
	
	We find some drawbacks in their data process.
	The author processes the data and calculates some statistics for the whole seven days and then use the data to get the ramdon forest model.
	
	\hao{maybe we can put the detail discussion of their data process problems in data analysis part and delete the above sentence only leave the first one.}
	
	And in our own work,firstly we reprocess the provided data in more resonable ways and use the same method (random forest) mentioned by the author to compare the results. Secondly, two other machine learning methods , xgboost and LSTM, are applied to get better performance.
	
	\section{Related Works}
	1. introduce the original paper
	In the original paper, the problem they solve is to use random forest model to predict three labels for one path: $(i)$ the remaining life time of a path (i.e., the time before a path changes), $(ii)$ the number of path changes in a future timeslot, and $(iii)$ the average RTT of a path in the next \texttt{traceroute} measurement.
	
	And to achieve these three targets, 69 input features are used to describe the  statistical properties of route dynamics and path latency.
	For the first route dynamic target, the first group of 11 features,referred to as $F_A$, is chosen. It describes the statistical properties (average, minimum, maximum, and percentiles) of the route duration observed for each path.
	And for the second target, the second group of 14 features, as $F_B$, are relevant to the prediction of number of route changes. $F_B$ features take into account the statistical properties of route changes. In addition to that, $F_B$ contains information about the number of route changes observed for a path so far and a binary feature indicating whether a route change occurred for a path in the current time slot. 
	And the last group of 44 features ,referred to as $F_C$ decribes statistical properties of path latency, which is relavent to the prediction of the next \texttt{traceroute} measurement. They calculate the staistical properties of four RTT metrics (average, minimum, maximum and standard deviation) reported from each \texttt{traceroute} measurement.
	
	Then they study the correlation among the input features and the targets and apply  feature selection techniques to select the best features for prediction. When using as input the full set of 69 input features $F_A \cup F_B \cup F_C$, and perform wrapper-based feature selection on top of this full set. The result shows that the top important features for three targets are not necessarily the ones in corresponding $F_A,F_B$ or $F_C$.
	
	And the authors use three different ways of selecting input features: $(i)$  use all of the 69 features to predict each target, $(ii)$ when predicting target $X$, use corresponding feature set $F_X$, $(iii)$ after wrapper-based feature selection, use the top features for each target. And finally, what they get is the third way of choosing input features can achieve better results (though there is only minor differences in output results).
	
		
	2. we can introduce some other machine learning methods applied in computer network scenarios. \hui{I'll take this part}
	
	
	3. very briefly introduce xgboost and lstm
	
	\section{Data Analysis}
	\hui{I think this is a important part, who will take this part?}
	
	1. We can plot some figures of the statistics of data, like the distribution of the route duration and avgRTT. 
	
	2. We can further discuss the relation between routes in one path or in different paths.
	
	3. Then we could discuss why the authors of the paper process the data in a wrong way
	
	4. We show our solution for data process. three ways for random forest models.
	
	\section{Methodol}
	\subsection{Data Processing}
	
	\subsection{LSTM Model}
	
	
	\section{Experiment}
	\subsection{Classic Models}
	
	We show the experiment results of the 3 different data we obtained, namely \textbf{K\&fix},\textbf{K\&update},\textbf{timeslot\&update}. We need to find some difference between our 3 data processing methods and the authors', i.e.,\textbf{origin}. 
	
	The experiments we need to conduct are as follows:
	\begin{itemize}
		
		\item 1. \textbf{K\&fix}+RF
		\item 2. \textbf{K\&update}+RF
		\item 3. \textbf{timeslot\&update}+RF
		\item 4. \textbf{origin}+RF
		\item 5. \textbf{K\&fix}+xgBoost
		\item 6. \textbf{K\&update}+xgBoost
		\item 7. \textbf{timeslot\&update}+xgBoost
		\item 8. \textbf{origin}+xgBoost
	\end{itemize}
	
	
	\hui{Note}We first predict the route duration instead of resLife. Further study on the difference between these two predicting objective could be conduct if we have time. Currently, we predict the route duration for task 1, because we can only predict this when we use LSTM.
	
	
	\subsection{Deep Models}
	
	There are two kinds of input for the LSTM at each timestep, (i) one simple scalar (ii) a vector.
	The experiments we need to conduct are as follows:
	\begin{itemize}
		\item scalar input + LSTM
		\item vector input + LSTM
	\end{itemize}
	
	\section{Conclusions}
	TODO
	
	
	
	\bibliographystyle{ACM-Reference-Format}
	\bibliography{bibliography} 
	
\end{document}

\documentclass[sigconf]{acmart}

\usepackage{booktabs} % For formal tables
\usepackage{url}
\usepackage{graphicx}
\usepackage{subcaption}
\newcommand{\kai}[1]{{\bf \color{blue} [[Shukai says ``#1'']]}}
\newcommand{\heng}[1]{{\bf \color{cyan} [[Yuheng says ``#1'']]}}
\newcommand{\hao}[1]{{\bf \color{red} [[Hao says ``#1'']]}}
\newcommand{\hui}[1]{{\bf \color{purple} [[Zhenghui says ``#1'']]}}
% Copyright
%\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


% DOI
%\acmDOI{10.475/123_4}

% ISBN
%\acmISBN{123-4567-24-567/08/06}

%Conference
\acmConference[CNW]{SJTU Computer Network Workshop}{December 2017}{Shanghai, China} 
\acmYear{2017}
\copyrightyear{2017}


%\acmArticle{4}
%\acmPrice{15.00}

% These commands are optional
%\acmBooktitle{Transactions of the ACM Woodstock conference}
%\editor{Jennifer B. Sartor}
%\editor{Theo D'Hondt}
%\editor{Wolfgang De Meuter}


\begin{document}
\title{Predicting Internet Path Dynamics and Performance with Machine Learning}
%\titlenote{Produces the permission block, and
%  copyright information}
%\subtitle{Extended Abstract}
%\subtitlenote{The full version of the author's guide is available as
%  \texttt{acmart.pdf} document}


\author{Zhenghui Wang}
\affiliation{%
  \institution{Shanghai Jiao Tong University}
  \city{Shanghai} 
  \state{China} 
  \postcode{200240}
}
\email{felixwzh@outlook.com}

\author{Hao Wang}
\affiliation{%
	\institution{Shanghai Jiao Tong University}
	\city{Shanghai} 
	\state{China} 
	\postcode{200240}
}
\email{?@?.com}

\author{Yuheng Zhi}
\affiliation{%
	\institution{Shanghai Jiao Tong University}
	\city{Shanghai} 
	\state{China} 
	\postcode{200240}
}
\email{?@?.com}

\author{Shukai Liu}
\affiliation{%
	\institution{Shanghai Jiao Tong University}
	\city{Shanghai} 
	\state{China} 
	\postcode{200240}
}
\email{?@?.com}

% The default list of authors is too long for headers.
\renewcommand{\shortauthors}{CN Group}


\begin{abstract}
We study the problem of predicting internet path dynamics and performance. We use \texttt{traceroute} measurement and machine learning models.
\end{abstract}

%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below. 
%
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10010520.10010553.10010562</concept_id>
  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010575.10010755</concept_id>
  <concept_desc>Computer systems organization~Redundancy</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010553.10010554</concept_id>
  <concept_desc>Computer systems organization~Robotics</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>10003033.10003083.10003095</concept_id>
  <concept_desc>Networks~Network reliability</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>  
\end{CCSXML}

%\ccsdesc[500]{Computer systems organization~Embedded systems}
%\ccsdesc[300]{Computer systems organization~Redundancy}
%\ccsdesc{Computer systems organization~Robotics}
%\ccsdesc[100]{Networks~Network reliability}


\keywords{TODO}


\maketitle
\hui{I think we should focus on the process not the final result, which is also important.}
\section{Introduction}

1. introduce the original paper: their task, method, and dataset

2. we find some drawbacks in their data. For instance, the route change times problem discussed in github. And the way they process the data.

3. we try some other models to get better performance like xgboost and LSTM.

\section{Related Works}
1. we introduce the original paper in details

2. we can introduce some other machine learning methods applied in computer network scenarios. \hui{I'll take this part}

3. very briefly introduce xgboost and lstm

\section{Data Analysis}
\hui{I think this is an important part, who will take this part?}
\heng{Plots of temporally differentiated prediction targets are given here to show their behavior patterns in temporal domain, thus supporting the use of LSTMs.}
With careful review of the data used in the original paper, here we (a) show the authors had used the data incorrectly in the context of making \textbf{predictions} and (b) reveal the temporal properties that the original method failed to employ but perfectly fits LSTMs (long short term memory neural networks), which we later use to improve the performance.

\subsection{Justification of Data Processing}
The original paper \cite{predict} made a major mistake on data employment: they implicitly used data from the future to predict events in the future, which is not possible. More specifically, all of their tests on the performance of \textit{NetPerfTrace} depend on statistics calculated out of data of all time, including the data they were to predict. Features like \{5, 15, 25...\} percentage point of route life, for example, takes the route life of all routes during the whole week into account, including route $r_i$. However, these features still take part in predicting the residual life of $r_i$. This is just like guessing something when you knew it.

In fact, a principle of making predictions is to use the data \textbf{already observed} to guess the unobserved. Thus, in all the experiments of this paper, we made it a hard constraint that all the features used to predict any event at, say, time $t$, only come from the raw data collected \textbf{before} $t$. First, we re-validated the performance of classic machine learning methods on the 3 tasks according to our justified data processing manner, in the following 3 ways: (a) use the global features (max value, min value, distribution, etc.) of the former k\% data to train the models and predict the targets of the latter (1-k)\% data, (b) use the global features to train the models and update them during the predicting procedure and (c) use data in a certain recent time-slot to calculate the global features. Then we train LSTMs on these data in two different ways: (a) input only a scalar, the prediction target, to the network and (b) input the prediction target and the global features as a vector.

\subsection{Temporal Properties of the Data}
We believe temporally adjacent routes and \textbf{traceroute} records of a path share some common network nodes, thus their behavior contain implicit dependencies in temporal domain. While the data are organized in the order of time, the original method ignored this property but regard them as independent instances, i.e., the prediction for the next time step only depends on the features of the present, but does not use information given by more previous time steps. This manner fails the original method to capture even the most naive temporal dependencies. Here we do not provide strict proofs of the temporal dependencies that lie in these routes but simply throw some light on the possibility of their existence.

\begin{figure}[tp]
	\centering
	\begin{subfigure}[t]{0.4\linewidth}
		\centering
		\includegraphics[width=0.95\linewidth, trim={100 100 0 0}, clip=true]{fig/total_life_8}
		\label{fig:3.1a}
	\end{subfigure}
	\begin{subfigure}[t]{0.4\linewidth}
		\centering
		\includegraphics[width=0.95\linewidth, trim={100 100 0 0}, clip]{fig/total_life_20}
		\label{fig:3.1b}
	\end{subfigure}
	\begin{subfigure}[t]{0.4\linewidth}
		\centering
		\includegraphics[width=0.95\linewidth, trim={100 100 0 0}, clip=true]{fig/total_life_21}
		\label{fig:3.1c}
	\end{subfigure}
	\begin{subfigure}[t]{0.4\linewidth}
		\centering
		\includegraphics[width=0.95\linewidth, trim={100 100 0 0}, clip]{fig/total_life_30}
		\label{fig:3.1d}
	\end{subfigure}
	\caption{Temporal patterns of route life among 4 different paths.}
	\label{fig:3.1}
\end{figure}

\begin{figure}[!htp]
	\centering
	\begin{subfigure}[t]{0.4\linewidth}
		\centering
		\includegraphics[width=0.95\linewidth, trim={100 100 0 0}, clip=true]{fig/route_change_8}
		\label{fig:3.2a}
	\end{subfigure}
	\begin{subfigure}[t]{0.4\linewidth}
		\centering
		\includegraphics[width=0.95\linewidth, trim={100 100 0 0}, clip]{fig/route_change_20}
		\label{fig:3.2b}
	\end{subfigure}
	\begin{subfigure}[t]{0.4\linewidth}
		\centering
		\includegraphics[width=0.95\linewidth, trim={100 100 0 0}, clip=true]{fig/route_change_21}
		\label{fig:3.2c}
	\end{subfigure}
	\begin{subfigure}[t]{0.4\linewidth}
		\centering
		\includegraphics[width=0.95\linewidth, trim={100 100 0 0}, clip]{fig/route_change_30}
		\label{fig:3.2d}
	\end{subfigure}
	\caption{Temporal patterns of the number of route changes among 4 different paths.}
	\label{fig:3.2}
\end{figure}

\begin{figure}[!htp]
	\centering
	\begin{subfigure}[t]{0.4\linewidth}
		\centering
		\includegraphics[width=0.95\linewidth, trim={100 100 0 0}, clip=true]{fig/RTT_8}
		\label{fig:3.3a}
	\end{subfigure}
	\begin{subfigure}[t]{0.4\linewidth}
		\centering
		\includegraphics[width=0.95\linewidth, trim={100 100 0 0}, clip]{fig/RTT_20}
		\label{fig:3.3b}
	\end{subfigure}
	\begin{subfigure}[t]{0.4\linewidth}
		\centering
		\includegraphics[width=0.95\linewidth, trim={100 100 0 0}, clip=true]{fig/RTT_21}
		\label{fig:3.3c}
	\end{subfigure}
	\begin{subfigure}[t]{0.4\linewidth}
		\centering
		\includegraphics[width=0.95\linewidth, trim={100 100 0 0}, clip]{fig/RTT_30}
		\label{fig:3.3d}
	\end{subfigure}
	\caption{Temporal patterns of round trip time among 4 different paths.}
	\label{fig:3.3}
\end{figure}

Applying simple differential analysis to the raw data, we found the prediction targets indeed show local temporal patterns which we can hopefully capture with LSTMs. Fig.~\ref{fig:3.1} to Fig.~\ref{fig:3.3} exhibit differentiated results of the three prediction targets in different paths. We can see that these paths behave in somehow consistent temporal patterns along their own ways but differently from each other.

No more solid evidence could be provided to prove the periodicity of these patterns because \textbf{pattern} itself is hard to define. However, these results are already enough to make us hopeful about the performance of LSTMs, which are expert in capturing temporal dependencies, no-matter long-term ones or short-term ones.


1. We can plot some figures of the statistics of data, like the distribution of the route duration and avgRTT. 

2. We can further discuss the relation between routes in one path or in different paths.

3. Then we could discuss why the authors of the paper process the data in a wrong way

4. We show our solution for data process. three ways for random forest models.

\section{Experiment}
\subsection{Classic Models}
We show the experiment results of the 3 different data we obtained, namely \textbf{K\&fix},\textbf{K\&update},\textbf{timeslot\&update}. We need to find some difference between our 3 data processing methods and the authors', i.e.,\textbf{origin}. 

The experiments we need to conduct are as follows:
\begin{itemize}

\item 1. \textbf{K\&fix}+RF
\item 2. \textbf{K\&update}+RF
\item 3. \textbf{timeslot\&update}+RF
\item 4. \textbf{origin}+RF
\item 5. \textbf{K\&fix}+xgBoost
\item 6. \textbf{K\&update}+xgBoost
\item 7. \textbf{timeslot\&update}+xgBoost
\item 8. \textbf{origin}+xgBoost
\end{itemize}


\hui{Note}We first predict the route duration instead of resLife. Further study on the difference between these two predicting objective could be conduct if we have time. Currently, we predict the route duration for task 1, because we can only predict this when we use LSTM.


\subsection{Deep Models}

There are two kinds of input for the LSTM at each timestep, (i) one simple scalar (ii) a vector.
The experiments we need to conduct are as follows:
\begin{itemize}
\item scalar input + LSTM
\item vector input + LSTM
\end{itemize}

\section{Conclusions}
TODO

\begin{acks}
TODO
\end{acks}



\bibliographystyle{ACM-Reference-Format}
\bibliography{bibliography} 

\end{document}
